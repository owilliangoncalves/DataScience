# Algoritmo de PCA (Principal Components Analysis)

√â uma t√©cnica de redu√ß√£o de dimensionalidade onde o objetivo principal √© transformar um conjunto de vari√°veis POSSIVELMENTE CORRELACIONADAS em um conjunto de valores de vari√°veis LINEARMENTE N√ÉO CORRELACIONADAS. Este processo √© realizado mantendo-se o m√°ximo poss√≠vel da variabilidade presente no conjunto de dados.
Aqui voc√™ encontrara 3 vers√µes do algoritmo PCA sendo a v1 a mais dif√≠cil (mais linhas de c√≥digo) e v3 a vers√£o com menos linhas de c√≥digo, todas as vers√µes usando o mesmo dataset.

Reveja o que s√£o autovalores e autovetores [aqui](autovalores_e_autovetores.md).

Reveja as propriedades de autovalores e autovetores [aqui](propiedades_autovalores_e_autovetores.md).

## O funcionamento do algoritmo de PCA pode ser descrito da seguinte maneira:

- **Normaliza√ß√£o dos dados:** Inicialmente os dados s√£o normalmente padronizados, especialmente se as vari√°veis tem unidades de medidas diferentes.

- **C√°lculo da matriz de covari√¢ncia:** A matriz de covari√¢ncia √© calculada para entender como as vari√°veis do conjunto de dados est√£o variando em rela√ß√£o uma √†s outras.

- **Determina√ß√£o de autovalores e autovetores:** S√£o calculados a partir da MATRIZ DE COVARI√ÇNCIA. Os AUTOVALORES indicam a quantidade de vari√¢ncia que pode ser atribu√≠da a cada AUTOVETOR.

- **Sele√ß√£o de componentes principais:** S√£o selecionados com base na quantidade de vari√¢ncia que eles representam. Geralmente s√£o escolhidos os componentes que somam a maior parte da vari√¢ncia total e isso permite um representa√ß√£o simplificada do conjunto de dados.

- **Transforma√ß√£o dos dados:** Os dados originais s√£o transformados em um novo conjunto de dados com base nos componentes principais selecionados.

## Como o PCA funciona?

O PCA baseia-se nos conceitos de Matriz de Vari√¢ncia e Covari√¢ncia.

- **Vari√¢ncia:** √â uma medida de variabilidade dos dados. Matematicamente √© o desvio quadr√°tico m√©dio da pontua√ß√£o m√©dia.
  > vari√¢ncia populacional

$$\sigma^2 = \frac{\sum_{i=1}^{n} (x_i - \mu)^2}{n}$$

Ambas as formas s√£o equivalentes e corretas!

onde:

- $\sigma^2$ √© a vari√¢ncia
- $n$ √© o n√∫mero de observa√ß√µes
- $x_i$ √© a i-√©sima observa√ß√£o
- $\mu$ √© a m√©dia da popula√ß√£o

---

- **Covari√¢ncia:** √â uma medida da extens√£o em que os elementos correspondentes de dois conjuntos de dados **ordenados** se movem na mesma dire√ß√£o, ou seja, como dois conjuntos de dados est√£o relacionados

> covari√¢ncia populacional

Aqui est√° a f√≥rmula da covari√¢ncia:

$$\text{Cov}(X, Y) = \frac{\sum_{i=1}^{n} (x_i - \mu_X)(y_i - \mu_Y)}{n}$$

Onde:

- $\text{Cov}(X, Y)$ √© a covari√¢ncia entre as vari√°veis $X$ e $Y$
- $n$ √© o n√∫mero de observa√ß√µes
- $x_i$ e $y_i$ s√£o as i-√©simas observa√ß√µes das vari√°veis $X$ e $Y$, respectivamente
- $\mu_X$ e $\mu_Y$ s√£o as m√©dias das vari√°veis $X$ e $Y$, respectivamente

---

## Adendo

No algoritmo PCA precisamos calcular uma matriz que resume como todas as vari√°veis se relacionam. Depois, dividimos essa matriz em dois componentes principais: **dire√ß√£o** e **magnitude**.

---

## Descri√ß√£o do algoritmo PCA

1. Seus dados de entrada devem estar divididos em uma ou mais colunas de entrada (representado por X) e uma ou mais colunas de sa√≠da (representado por Y). Aplicamos o PCA em X. Se os seus dados tiverem a vari√°vel de sa√≠da Y, ela n√£o entra no c√°lculo do PCA. Nesse caso voc√™ reduziria a dimensionalidade de X para ent√£o tentar prever Y. PCA trabalha no X.

2. Pegue a matriz de vari√°veis independentes X e para cada coluna, subtraia a m√©dia dessa coluna de cada entrada (Isso garante que cada coluna tenha uma m√©dia de zero).

3. Decida se deve ou n√£o padronizar. Dadas as colunas de X os recursos com varia√ß√£o mais alta s√£o mais importantes em rela√ß√£o aos recursos com varia√ß√£o menor ou a import√¢ncia dos recursos √© independente da varia√ß√£o?

4. Pegue a matriz Z, transponha e multiplique a matriz transposta por Z (Escrevendo isso matematicamente, ter√≠amos Z·µÄZ.) A matriz resultante √© a matriz de covari√¢ncia de Z.

5. Calcule os autovetores e seus autovalores correspondentes de Z. A composi√ß√£o autom√°tica de Z·µÄZ √© onde decompomos Z·µÄZ em PDP‚Åª¬π, onde P √© a matriz de autovetores e D √© a matriz diagonal com autovalores na diagonal e valores de zero em qualquer outro lugar. Os autovalores na diagonal de D ser√£o associados √† coluna correspondente em P. Ou seja, o primeiro elemento de D √© Œª‚ÇÅ e o autovetor correspondente √© a primeira coluna de P. Isso vale para todos os elementos em D e seus autovetores correspondentes em P. Sempre poderemos calcular o PDP‚Åª¬π dessa maneira.

6. Pegue os autovalores Œª‚ÇÅ, Œª‚ÇÇ,‚Ä¶, Œªn e classifique-os do maior para o menor. Ao fazer isso, classifique os autovetores em P. Por exemplo, se Œª‚ÇÇ √© o maior autovalor, pegue a segunda coluna de P e coloque-a na posi√ß√£o da primeira coluna. Dependendo do pacote de computa√ß√£o, isso pode ser feito automaticamente. Chame essa matriz classificada de autovetores P* (As colunas de P* devem ser as mesmas que as de P, mas talvez em uma ordem diferente). Observe que esses autovetores s√£o independentes um do outro.

7. Calcular Z* = ZP*. Essa nova matriz, Z*, √© uma vers√£o centralizada/padronizada de X, mas agora cada observa√ß√£o √© uma combina√ß√£o das vari√°veis originais, onde os pesos s√£o determinados pelo autovetor. Como um b√¥nus, porque nossos autovetores em P* s√£o independentes um do outro, cada coluna de Z\* tamb√©m √© independente uma da outra!

8. Finalmente, precisamos determinar quantos componentes principais manter versus quantos deixar de fora. Existem tr√™s m√©todos comuns para determinar isso, discutidos abaixo:

- M√©todo 1: Selecionamos arbitrariamente quantas dimens√µes queremos manter.

- M√©todo 2: Calculamos a propor√ß√£o de varia√ß√£o explicada para cada recurso, escolhemos um limite e adicionamos recursos at√© atingir esse limite (Por exemplo, se voc√™ deseja explicar 80% da variabilidade total possivelmente explicada pelo seu modelo, adicionamos recursos com a maior propor√ß√£o de varia√ß√£o explicada at√© que a propor√ß√£o de varia√ß√£o explicada atinja ou exceda 80%). **Este √© o m√©todo ideal** para o caso que estamos trabalhando.

- M√©todo 3: Este est√° intimamente relacionado ao m√©todo 2. Calculamos a propor√ß√£o de varia√ß√£o explicada para cada recurso, classificamos os recursos por propor√ß√£o de varia√ß√£o explicada e plotamos a propor√ß√£o acumulada de varia√ß√£o explicada √† medida que mantemos mais recursos. √â poss√≠vel escolher quantos recursos incluir, identificando o ponto em que a adi√ß√£o de um novo recurso tem uma queda significativa na varia√ß√£o explicada em rela√ß√£o ao recurso anterior e a escolha de recursos at√© esse ponto (Chamamos isso de m√©todo ‚Äúencontre o cotovelo‚Äù, pois olhar para a ‚Äúcurva‚Äù ou ‚Äúcotovelo‚Äù no gr√°fico determina onde ocorre a maior queda na propor√ß√£o da varia√ß√£o explicada).

Como cada autovalor √© aproximadamente a import√¢ncia do seu autovetor correspondente, a propor√ß√£o de varia√ß√£o explicada √© a soma dos autovalores dos recursos que voc√™ manteve divididos pela soma dos autovalores de todos os recursos.

Os autovetores da matriz de covari√¢ncia s√£o as dire√ß√µes principais, enquanto os autovalores representam a magnitude dessas dire√ß√µes. Os autovalores s√£o importantes para entender a quantidade de varia√ß√£o capturada por cada componente principal.

#### Vari√¢ncia e Vari√¢ncia Acumulada nos Componentes Principais:

## üõ†Ô∏è Constru√≠do com:

- [NumPy](https://numpy.org/pt/) - NumPy
- [Pandas](https://pandas.pydata.org/) - Pandas
- [Scikit-learn] (https://scikit-learn.org/stable/) - Scikit-learn
